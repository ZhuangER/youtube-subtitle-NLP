{
 "metadata": {
  "name": "",
  "signature": "sha256:d45d211306366f9b95253b8378ad34e3472009d52d5459cc723acdaa0625a5cc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Named Entity Tagger\n",
      "main task is to detect whether the text contain special entity, such as following 7 classes:\n",
      "- location\n",
      "- organization\n",
      "- date\n",
      "- money\n",
      "- person\n",
      "- percent\n",
      "- time\n",
      "\n",
      "Online demo: http://nlp.stanford.edu:8080/ner/process\n",
      "\n",
      "Reference\n",
      "- http://nlp.stanford.edu/software/CRF-NER.shtml"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk \n",
      "# with open('sample.txt', 'r') as f:\n",
      "#     sample = f.read()\n",
      "\n",
      "sample = \"in my own language.\\\n",
      "As a video uploader, this means you can reach\\\n",
      "to people all over the world,\\\n",
      "irrespective of language.\\\n",
      "[Hiroto, Bedhead]\\\n",
      "You can upload multiple tracks like English and French,\\\n",
      "and viewers can choose the track they like.\\\n",
      "[Toliver, Japanese Learner]\\\n",
      "For example, if you enjoy using YouTube in French,\"\n",
      "\n",
      "sentences = nltk.sent_tokenize(sample)\n",
      "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
      "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
      "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
      "\n",
      "def extract_entity_names(t):\n",
      "    entity_names = []\n",
      "\n",
      "    if hasattr(t, 'label') and t.label:\n",
      "        if t.label() == 'NE':\n",
      "            entity_names.append(' '.join([child[0] for child in t]))\n",
      "        else:\n",
      "            for child in t:\n",
      "                entity_names.extend(extract_entity_names(child))\n",
      "\n",
      "    return entity_names\n",
      "\n",
      "entity_names = []\n",
      "for tree in chunked_sentences:\n",
      "    # Print results per sentence\n",
      "    # print extract_entity_names(tree)\n",
      "\n",
      "    entity_names.extend(extract_entity_names(tree))\n",
      "\n",
      "# Print all entity names\n",
      "#print entity_names\n",
      "\n",
      "# Print unique entity names\n",
      "print set(entity_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['Bedhead', 'YouTube', 'French', 'English', 'Hiroto', 'Japanese Learner'])\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Stanford NER\n",
      "\n",
      "Install java 8, download Stanford NER, and set the environment variable first.\n",
      "\n",
      "NLTK just provide an interface of Stanford NER"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tag import StanfordNERTagger\n",
      "st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \n",
      "st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LookupError",
       "evalue": "\n\n===========================================================================\n  NLTK was unable to find stanford-ner.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-ner.jar, see:\n    <http://nlp.stanford.edu/software>\n===========================================================================",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-11-adc41096c38f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english.all.3class.distsim.crf.ser.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Rami Eid is studying at Stony Brook University in NY'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/tag/stanford.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/tag/stanford.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0msearchpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 verbose=verbose)\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         self._stanford_model = find_file(model_filename,\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mfind_jar\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    717\u001b[0m         searchpath=(), url=None, verbose=True, is_regex=False):\n\u001b[0;32m    718\u001b[0m     return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[1;32m--> 719\u001b[1;33m                          searchpath, url, verbose, is_regex))\n\u001b[0m\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    712\u001b[0m                     (name_pattern, url))\n\u001b[0;32m    713\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m def find_jar(name_pattern, path_to_jar=None, env_vars=(),\n",
        "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\n  NLTK was unable to find stanford-ner.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-ner.jar, see:\n    <http://nlp.stanford.edu/software>\n==========================================================================="
       ]
      }
     ],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}