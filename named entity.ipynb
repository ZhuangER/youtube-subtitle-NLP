{
 "metadata": {
  "name": "",
  "signature": "sha256:bdb8e2b0f7ffbe576d01c6e192d2f0e5d8b64d70979d45ae0f537557715c386a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk \n",
      "# with open('sample.txt', 'r') as f:\n",
      "#     sample = f.read()\n",
      "\n",
      "sample = \"in my own language.\\\n",
      "As a video uploader, this means you can reach\\\n",
      "to people all over the world,\\\n",
      "irrespective of language.\\\n",
      "[Hiroto, Bedhead]\\\n",
      "You can upload multiple tracks like English and French,\\\n",
      "and viewers can choose the track they like.\\\n",
      "[Toliver, Japanese Learner]\\\n",
      "For example, if you enjoy using YouTube in French,\"\n",
      "\n",
      "sentences = nltk.sent_tokenize(sample)\n",
      "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
      "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
      "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
      "\n",
      "def extract_entity_names(t):\n",
      "    entity_names = []\n",
      "\n",
      "    if hasattr(t, 'label') and t.label:\n",
      "        if t.label() == 'NE':\n",
      "            entity_names.append(' '.join([child[0] for child in t]))\n",
      "        else:\n",
      "            for child in t:\n",
      "                entity_names.extend(extract_entity_names(child))\n",
      "\n",
      "    return entity_names\n",
      "\n",
      "entity_names = []\n",
      "for tree in chunked_sentences:\n",
      "    # Print results per sentence\n",
      "    # print extract_entity_names(tree)\n",
      "\n",
      "    entity_names.extend(extract_entity_names(tree))\n",
      "\n",
      "# Print all entity names\n",
      "#print entity_names\n",
      "\n",
      "# Print unique entity names\n",
      "print set(entity_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['Bedhead', 'YouTube', 'French', 'English', 'Hiroto', 'Japanese Learner'])\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# RAKE Algorithm\n",
      "\n",
      "tend to extract longer phrase  \n",
      "a phrase almost not contain **punctuations** and **stop words**, thus split the sentence based on the punctuations and stop words.  \n",
      "\n",
      "Score of each phrase  \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Adapted from: github.com/aneesha/RAKE/rake.py\n",
      "from __future__ import division\n",
      "import operator\n",
      "import nltk\n",
      "import string\n",
      "\n",
      "def isPunct(word):\n",
      "  return len(word) == 1 and word in string.punctuation\n",
      "\n",
      "def isNumeric(word):\n",
      "  try:\n",
      "    float(word) if '.' in word else int(word)\n",
      "    return True\n",
      "  except ValueError:\n",
      "    return False\n",
      "\n",
      "class RakeKeywordExtractor:\n",
      "\n",
      "  def __init__(self):\n",
      "    self.stopwords = set(nltk.corpus.stopwords.words())\n",
      "    self.top_fraction = 1 # consider top third candidate keywords by score\n",
      "\n",
      "  def _generate_candidate_keywords(self, sentences):\n",
      "    phrase_list = []\n",
      "    for sentence in sentences:\n",
      "      words = map(lambda x: \"|\" if x in self.stopwords else x,\n",
      "        nltk.word_tokenize(sentence.lower()))\n",
      "      phrase = []\n",
      "      for word in words:\n",
      "        if word == \"|\" or isPunct(word):\n",
      "          if len(phrase) > 0:\n",
      "            phrase_list.append(phrase)\n",
      "            phrase = []\n",
      "        else:\n",
      "          phrase.append(word)\n",
      "    return phrase_list\n",
      "\n",
      "  def _calculate_word_scores(self, phrase_list):\n",
      "    word_freq = nltk.FreqDist()\n",
      "    word_degree = nltk.FreqDist()\n",
      "    for phrase in phrase_list:\n",
      "      degree = len(filter(lambda x: not isNumeric(x), phrase)) - 1\n",
      "      for word in phrase:\n",
      "        word_freq[word] += 1\n",
      "        word_degree[word] += 1 # other words\n",
      "    for word in word_freq.keys():\n",
      "      word_degree[word] = word_degree[word] + word_freq[word] # itself\n",
      "    # word score = deg(w) / freq(w)\n",
      "    word_scores = {}\n",
      "    for word in word_freq.keys():\n",
      "      word_scores[word] = word_degree[word] / word_freq[word]\n",
      "    return word_scores\n",
      "\n",
      "  def _calculate_phrase_scores(self, phrase_list, word_scores):\n",
      "    phrase_scores = {}\n",
      "    for phrase in phrase_list:\n",
      "      phrase_score = 0\n",
      "      for word in phrase:\n",
      "        phrase_score += word_scores[word]\n",
      "      phrase_scores[\" \".join(phrase)] = phrase_score\n",
      "    return phrase_scores\n",
      "    \n",
      "  def extract(self, text, incl_scores=False):\n",
      "    sentences = nltk.sent_tokenize(text)\n",
      "    phrase_list = self._generate_candidate_keywords(sentences)\n",
      "    word_scores = self._calculate_word_scores(phrase_list)\n",
      "    phrase_scores = self._calculate_phrase_scores(\n",
      "      phrase_list, word_scores)\n",
      "    sorted_phrase_scores = sorted(phrase_scores.iteritems(),\n",
      "      key=operator.itemgetter(1), reverse=True)\n",
      "    n_phrases = len(sorted_phrase_scores)\n",
      "    if incl_scores:\n",
      "      return sorted_phrase_scores[0:int(n_phrases/self.top_fraction)]\n",
      "    else:\n",
      "      return map(lambda x: x[0],\n",
      "        sorted_phrase_scores[0:int(n_phrases/self.top_fraction)])\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test():\n",
      "  rake = RakeKeywordExtractor()\n",
      "  keywords = rake.extract(\"\"\"\n",
      "Compatibility of systems of linear constraints over the set of natural \n",
      "numbers. Criteria of compatibility of a system of linear Diophantine \n",
      "equations, strict inequations, and nonstrict inequations are considered. \n",
      "Upper bounds for components of a minimal set of solutions and algorithms \n",
      "of construction of minimal generating sets of solutions for all types of \n",
      "systems are given. These criteria and the corresponding algorithms for \n",
      "constructing a minimal supporting set of solutions can be used in solving \n",
      "all the considered types of systems and systems of mixed types.\n",
      "  \"\"\", incl_scores=True)\n",
      "  print keywords\n",
      "  \n",
      "    \n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('linear diophantine equations', 6.0), ('minimal supporting set', 6.0), ('minimal generating sets', 6.0), ('minimal set', 4.0), ('upper bounds', 4.0), ('linear constraints', 4.0), ('corresponding algorithms', 4.0), ('natural numbers', 4.0), ('nonstrict inequations', 4.0), ('considered types', 4.0), ('strict inequations', 4.0), ('mixed types', 4.0), ('set', 2.0), ('constructing', 2.0), ('solutions', 2.0), ('given', 2.0), ('solving', 2.0), ('system', 2.0), ('systems', 2.0), ('criteria', 2.0), ('compatibility', 2.0), ('used', 2.0), ('construction', 2.0), ('types', 2.0), ('considered', 2.0), ('algorithms', 2.0), ('components', 2.0)]\n"
       ]
      }
     ],
     "prompt_number": 6
    }
   ],
   "metadata": {}
  }
 ]
}