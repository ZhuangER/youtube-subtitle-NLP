{
 "metadata": {
  "name": "",
  "signature": "sha256:0fd368526eb4f13d63264fd31efa450a38405a635fc53945f340041cb7a9afa3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# RAKE Algorithm\n",
      "\n",
      "Rapid Automatic Keyword Extraction\n",
      "\n",
      "feature: domain-independent, language-independent\n",
      "\n",
      "tend to extract longer phrase  \n",
      "a phrase almost not contain **punctuations** and **stop words**, thus split the sentence based on the punctuations and stop words.  \n",
      "\n",
      "Score of each phrase:  \n",
      "$$wordScore = wordDegree(w) / wordFrequency(w)$$\n",
      "\n",
      "phraseScore equals the sum of wordScore in the phrase\n",
      "\n",
      "\n",
      "Problem of RAKE algorithm:\n",
      "- may produce long phrases"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Adapted from: github.com/aneesha/RAKE/rake.py\n",
      "from __future__ import division\n",
      "import operator\n",
      "import nltk\n",
      "import string\n",
      "\n",
      "def isPunct(word):\n",
      "  return len(word) == 1 and word in string.punctuation\n",
      "\n",
      "def isNumeric(word):\n",
      "  try:\n",
      "    float(word) if '.' in word else int(word)\n",
      "    return True\n",
      "  except ValueError:\n",
      "    return False\n",
      "\n",
      "class RakeKeywordExtractor:\n",
      "\n",
      "  def __init__(self):\n",
      "    self.stopwords = set(nltk.corpus.stopwords.words())\n",
      "    self.top_fraction = 1 # consider top third candidate keywords by score\n",
      "\n",
      "  def _generate_candidate_keywords(self, sentences):\n",
      "    phrase_list = []\n",
      "    for sentence in sentences:\n",
      "      words = map(lambda x: \"|\" if x in self.stopwords else x,\n",
      "                    nltk.word_tokenize(sentence.lower()))\n",
      "      phrase = []\n",
      "      for word in words:\n",
      "        if word == \"|\" or isPunct(word):\n",
      "          if len(phrase) > 0:\n",
      "            phrase_list.append(phrase)\n",
      "            phrase = []\n",
      "        else:\n",
      "          phrase.append(word)\n",
      "    return phrase_list\n",
      "\n",
      "  def _calculate_word_scores(self, phrase_list):\n",
      "    word_freq = nltk.FreqDist()\n",
      "    word_degree = nltk.FreqDist()\n",
      "    for phrase in phrase_list:\n",
      "      word_list_degree = len(filter(lambda x: not isNumeric(x), phrase)) - 1\n",
      "      for word in phrase:\n",
      "        word_freq[word] += 1\n",
      "        word_degree[word] += word_list_degree # other words\n",
      "    for word in word_freq.keys():\n",
      "      word_degree[word] = word_degree[word] + word_freq[word] # itself\n",
      "    # word score = deg(w) / freq(w)\n",
      "    word_scores = {}\n",
      "    for word in word_freq.keys():\n",
      "      word_scores[word] = word_degree[word] / word_freq[word]\n",
      "    return word_scores\n",
      "\n",
      "  def _calculate_phrase_scores(self, phrase_list, word_scores):\n",
      "    phrase_scores = {}\n",
      "    for phrase in phrase_list:\n",
      "      phrase_score = 0\n",
      "      for word in phrase:\n",
      "        phrase_score += word_scores[word]\n",
      "      phrase_scores[\" \".join(phrase)] = phrase_score\n",
      "    return phrase_scores\n",
      "    \n",
      "  def extract(self, text, incl_scores=False):\n",
      "    sentences = nltk.sent_tokenize(text)\n",
      "    phrase_list = self._generate_candidate_keywords(sentences)\n",
      "    word_scores = self._calculate_word_scores(phrase_list)\n",
      "    phrase_scores = self._calculate_phrase_scores(phrase_list, word_scores)\n",
      "    sorted_phrase_scores = sorted(phrase_scores.iteritems(), \n",
      "                                  key=operator.itemgetter(1), reverse=True)\n",
      "    n_phrases = len(sorted_phrase_scores)\n",
      "    if incl_scores:\n",
      "      return sorted_phrase_scores[0:int(n_phrases/self.top_fraction)]\n",
      "    else:\n",
      "      return map(lambda x: x[0],\n",
      "        sorted_phrase_scores[0:int(n_phrases/self.top_fraction)])\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test():\n",
      "  rake = RakeKeywordExtractor()\n",
      "  keywords = rake.extract(\"\"\"\n",
      "Compatibility of systems of linear constraints over the set of natural \n",
      "numbers. Criteria of compatibility of a system of linear Diophantine \n",
      "equations, strict inequations, and nonstrict inequations are considered. \n",
      "Upper bounds for components of a minimal set of solutions and algorithms \n",
      "of construction of minimal generating sets of solutions for all types of \n",
      "systems are given. These criteria and the corresponding algorithms for \n",
      "constructing a minimal supporting set of solutions can be used in solving \n",
      "all the considered types of systems and systems of mixed types.\n",
      "  \"\"\", incl_scores=True)\n",
      "  print keywords\n",
      "  \n",
      "    \n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('minimal generating sets', 8.666666666666666), ('linear diophantine equations', 8.5), ('minimal supporting set', 7.666666666666666), ('minimal set', 4.666666666666666), ('linear constraints', 4.5), ('upper bounds', 4.0), ('natural numbers', 4.0), ('nonstrict inequations', 4.0), ('strict inequations', 4.0), ('mixed types', 3.666666666666667), ('corresponding algorithms', 3.5), ('considered types', 3.166666666666667), ('set', 2.0), ('types', 1.6666666666666667), ('considered', 1.5), ('algorithms', 1.5), ('constructing', 1.0), ('solutions', 1.0), ('given', 1.0), ('solving', 1.0), ('system', 1.0), ('systems', 1.0), ('criteria', 1.0), ('compatibility', 1.0), ('used', 1.0), ('construction', 1.0), ('components', 1.0)]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample = \"in my own language.\\\n",
      "As a video uploader, this means you can reach\\\n",
      "to people all over the world,\\\n",
      "irrespective of language.\\\n",
      "[Hiroto, Bedhead]\\\n",
      "You can upload multiple tracks like English and French,\\\n",
      "and viewers can choose the track they like.\\\n",
      "[Toliver, Japanese Learner]\\\n",
      "For example, if you enjoy using YouTube in French,\"\n",
      "rake = RakeKeywordExtractor()\n",
      "keywords = rake.extract(sample, incl_scores=True)\n",
      "\n",
      "print keywords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('upload multiple tracks like english', 23.0), ('enjoy using youtube', 9.0), ('video uploader', 4.0), ('reachto people', 4.0), ('japanese learner', 4.0), ('like', 3.0), ('toliver', 1.0), ('language', 1.0), ('means', 1.0), ('bedhead', 1.0), ('choose', 1.0), ('french', 1.0), ('track', 1.0), ('hiroto', 1.0), ('irrespective', 1.0), ('world', 1.0), ('example', 1.0), ('viewers', 1.0), ('language.as', 1.0)]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rake = RakeKeywordExtractor()\n",
      "keywords = rake.extract(test_article, incl_scores=True)\n",
      "\n",
      "print keywords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'console wars got even hotter today', 33.0), (u'\\u2019\\u201d said zenith ceo michael ahn', 32.8), (u'playstation 4 hitting stores later', 19.0), (u'next-generation video game systems', 15.333333333333334), (u'video game world.\\u201d according', 15.333333333333334), (u'system\\u2019s launch titles moonchaser', 15.0), (u'electronics manufacturer zenith announced', 14.8), (u'gamespace pro press event', 13.333333333333332), (u'cris collinsworth\\u2019s pigskin 2013', 12.0), (u'nine launch titles', 10.0), (u'sleek silver-and-gray box', 9.0), (u'survival-horror thriller inzomnia', 9.0), (u'stores nov. 19', 7.0), (u'console game', 6.333333333333334), (u'gamespace pro', 5.333333333333333), (u'zenith representatives', 4.8), (u'play cds', 4.0), (u'z-connect technology', 4.0), (u'finally poised', 4.0), (u'starting price', 4.0), (u'3d graphics', 4.0), (u'big waves', 4.0), (u'internet using', 4.0), (u'xbox one', 4.0), (u'double-analog-stick controllers', 4.0), (u'console', 3.0), (u'zenith', 2.8), (u'650 units', 2.0), (u'saying', 1.0), (u'already', 1.0), (u'month', 1.0), (u'player', 1.0), (u'\\u201cwith', 1.0), (u'make', 1.0), (u'radiation', 1.0), (u'preordered', 1.0), (u'way', 1.0), (u'officially', 1.0), (u'\\u2018move', 1.0), (u'arrives', 1.0), (u'ability', 1.0), (u'lincolnshire', 1.0), (u'release', 1.0), (u'sony', 1.0), (u'log', 1.0), (u'showcasing', 1.0), (u'microsoft', 1.0), (u'374.99', 0.0)]\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# TextRank Algorithm\n",
      "\n",
      "https://github.com/davidadamojr/TextRank\n",
      "\n",
      "graph-based ranking algorithm  \n",
      "rank words based on their associations in the graph  \n",
      "has best performance when only nouns and adjectives are selected as potential keywords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "From this paper: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
      "External dependencies: nltk, numpy, networkx\n",
      "Based on https://gist.github.com/voidfiles/1646117\n",
      "\"\"\"\n",
      "\n",
      "import nltk\n",
      "import itertools\n",
      "from operator import itemgetter\n",
      "import networkx as nx\n",
      "\n",
      "class TextRank:\n",
      "\n",
      "    def _normalize(self, tagged):\n",
      "        return [(item[0].replace('.', ''), item[1]) for item in tagged]\n",
      "\n",
      "    def _unique_everseen(self, iterable, key=None):\n",
      "        \"List unique elements, preserving order. Remember all elements ever seen.\"\n",
      "        # unique_everseen('AAAABBBCCDAABBB') --> A B C D\n",
      "        # unique_everseen('ABBCcAD', str.lower) --> A B C D\n",
      "        seen = set()\n",
      "        seen_add = seen.add\n",
      "        if key is None:\n",
      "            for element in itertools.ifilterfalse(seen.__contains__, iterable):\n",
      "                seen_add(element)\n",
      "                yield element\n",
      "        else:\n",
      "            for element in iterable:\n",
      "                k = key(element)\n",
      "                if k not in seen:\n",
      "                    seen_add(k)\n",
      "                    yield element\n",
      "\n",
      "    def _lDistance(self, firstString, secondString):\n",
      "        \"Function to find the Levenshtein distance between two words/sentences - gotten from http://rosettacode.org/wiki/Levenshtein_distance#Python\"\n",
      "        if len(firstString) > len(secondString):\n",
      "            firstString, secondString = secondString, firstString\n",
      "        distances = range(len(firstString) + 1)\n",
      "        for index2, char2 in enumerate(secondString):\n",
      "            newDistances = [index2 + 1]\n",
      "            for index1, char1 in enumerate(firstString):\n",
      "                if char1 == char2:\n",
      "                    newDistances.append(distances[index1])\n",
      "                else:\n",
      "                    newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1])))\n",
      "            distances = newDistances\n",
      "        return distances[-1]\n",
      "\n",
      "    def _buildGraph(self, nodes):\n",
      "        \"nodes - list of hashables that represents the nodes of the graph\"\n",
      "        gr = nx.Graph() #initialize an undirected graph\n",
      "        gr.add_nodes_from(nodes)\n",
      "        nodePairs = list(itertools.combinations(nodes, 2))\n",
      "\n",
      "        #add edges to the graph (weighted by Levenshtein distance)\n",
      "        for pair in nodePairs:\n",
      "            firstString = pair[0]\n",
      "            secondString = pair[1]\n",
      "            levDistance = self._lDistance(firstString, secondString)\n",
      "            gr.add_edge(firstString, secondString, weight=levDistance)\n",
      "\n",
      "        return gr\n",
      "\n",
      "    def extractKeyphrases(self, text):\n",
      "        #tokenize the text using nltk\n",
      "        wordTokens = nltk.word_tokenize(text)\n",
      "\n",
      "        #assign POS tags to the words in the text\n",
      "        tagged = nltk.pos_tag(wordTokens)\n",
      "        textlist = [x[0] for x in tagged]\n",
      "\n",
      "        # filter words with 'NN', 'JJ', 'NNP' tags\n",
      "        tagged = filter(lambda x: x[1] in ['NN', 'JJ', 'NNP'], tagged)\n",
      "        tagged = self._normalize(tagged)\n",
      "\n",
      "        unique_word_set = self._unique_everseen([x[0] for x in tagged])\n",
      "        word_set_list = list(unique_word_set)\n",
      "\n",
      "       #this will be used to determine adjacent words in order to construct keyphrases with two words\n",
      "\n",
      "        graph = self._buildGraph(word_set_list)\n",
      "\n",
      "        #pageRank - initial value of 1.0, error tolerance of 0,0001, \n",
      "        calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
      "\n",
      "        #most important words in ascending order of importance\n",
      "        keyphrases = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
      "\n",
      "        #the number of keyphrases returned will be relative to the size of the text (a third of the number of vertices)\n",
      "        aThird = int(len(word_set_list) / 3)\n",
      "        keyphrases = keyphrases[0:aThird+1]\n",
      "\n",
      "        #take keyphrases with multiple words into consideration as done in the paper - if two words are adjacent in the text and are selected as keywords, join them\n",
      "        #together\n",
      "        modifiedKeyphrases = set([])\n",
      "        dealtWith = set([]) #keeps track of individual keywords that have been joined to form a keyphrase\n",
      "        i = 0\n",
      "        j = 1\n",
      "        while j < len(textlist):\n",
      "            firstWord = textlist[i]\n",
      "            secondWord = textlist[j]\n",
      "            if firstWord in keyphrases and secondWord in keyphrases:\n",
      "                keyphrase = firstWord + ' ' + secondWord\n",
      "                modifiedKeyphrases.add(keyphrase)\n",
      "                dealtWith.add(firstWord)\n",
      "                dealtWith.add(secondWord)\n",
      "            else:\n",
      "                if firstWord in keyphrases and firstWord not in dealtWith: \n",
      "                    modifiedKeyphrases.add(firstWord)\n",
      "\n",
      "                #if this is the last word in the text, and it is a keyword,\n",
      "                #it definitely has no chance of being a keyphrase at this point    \n",
      "                if j == len(textlist)-1 and secondWord in keyphrases and secondWord not in dealtWith:\n",
      "                    modifiedKeyphrases.add(secondWord)\n",
      "\n",
      "            i = i + 1\n",
      "            j = j + 1\n",
      "\n",
      "        return modifiedKeyphrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_article = \"LINCOLNSHIRE, IL With next-generation video game systems such as the Xbox One and the Playstation 4 hitting stores later this month, the console wars got even hotter today as electronics manufacturer Zenith announced the release of its own console, the Gamespace Pro, which arrives in stores Nov. 19. \u201cWith its sleek silver-and-gray box, double-analog-stick controllers, ability to play CDs, and starting price of $374.99, the Gamespace Pro is our way of saying, \u2018Move over, Sony and Microsoft, Zenith is now officially a player in the console game,\u2019\u201d said Zenith CEO Michael Ahn at a Gamespace Pro press event, showcasing the system\u2019s launch titles MoonChaser: Radiation, Cris Collinsworth\u2019s Pigskin 2013, and survival-horror thriller InZomnia. \u201cWith over nine launch titles, 3D graphics, and the ability to log on to the internet using our Z-Connect technology, Zenith is finally poised to make some big waves in the video game world.\u201d According to Zenith representatives, over 650 units have already been preordered.\"\n",
      "test_article = test_article.decode('utf-8')\n",
      "\n",
      "# test_list = test_article.split()\n",
      "textRank = TextRank()\n",
      "keyphrases = textRank.extractKeyphrases(test_article)\n",
      "print 'keyphrase list:'\n",
      "print keyphrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "keyphrase list:\n",
        "set([u'Playstation', u'survival-horror thriller', u'LINCOLNSHIRE', u'MoonChaser', u'Collinsworth\\u2019s', u'Radiation', u'Z-Connect technology', u'Microsoft', u'double-analog-stick', u'silver-and-gray', u'system\\u2019s', u'thriller InZomnia', u'internet', u'next-generation', u'Gamespace', u'manufacturer'])\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_candidate_chunks(text, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'):\n",
      "    import itertools, nltk, string\n",
      "    \n",
      "    # exclude candidates that are stop words or entirely punctuation\n",
      "    punct = set(string.punctuation)\n",
      "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
      "    # tokenize, POS-tag, and chunk using regular expressions\n",
      "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
      "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
      "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
      "                                                    for tagged_sent in tagged_sents))\n",
      "    # join constituent chunk words into a single chunked phrase\n",
      "    candidates = [' '.join(word for word, pos, chunk in group).lower()\n",
      "                  for key, group in itertools.groupby(all_chunks, lambda (word,pos,chunk): chunk != 'O') if key]\n",
      "    print candidates\n",
      "    return [cand for cand in candidates\n",
      "            if cand not in stop_words and not all(char in punct for char in cand)]\n",
      "\n",
      "def extract_candidate_words(text, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n",
      "    import itertools, nltk, string\n",
      "\n",
      "    # exclude candidates that are stop words or entirely punctuation\n",
      "    punct = set(string.punctuation)\n",
      "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
      "    # tokenize and POS-tag words\n",
      "    tagged_words = itertools.chain.from_iterable(nltk.pos_tag_sents(nltk.word_tokenize(sent)\n",
      "                                                                    for sent in nltk.sent_tokenize(text)))\n",
      "    # filter on certain POS tags and lowercase all words\n",
      "    candidates = [word.lower() for word, tag in tagged_words\n",
      "                  if tag in good_tags and word.lower() not in stop_words\n",
      "                  and not all(char in punct for char in word)]\n",
      "    return candidates\n",
      "\n",
      "def score_keyphrases_by_tfidf(texts, candidates='chunks'):\n",
      "    import gensim, nltk\n",
      "    \n",
      "    # extract candidates from each text in texts, either chunks or words\n",
      "    if candidates == 'chunks':\n",
      "        boc_texts = [extract_candidate_chunks(text) for text in texts]\n",
      "    elif candidates == 'words':\n",
      "        boc_texts = [extract_candidate_words(text) for text in texts]\n",
      "    # make gensim dictionary and corpus\n",
      "    dictionary = gensim.corpora.Dictionary(boc_texts)\n",
      "    corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]\n",
      "    # transform corpus with tf*idf model\n",
      "    tfidf = gensim.models.TfidfModel(corpus)\n",
      "    corpus_tfidf = tfidf[corpus]\n",
      "    \n",
      "    return corpus_tfidf, dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = \"\"\"\n",
      "Compatibility of systems of linear constraints over the set of natural \n",
      "numbers. Criteria of compatibility of a system of linear Diophantine \n",
      "equations, strict inequations, and nonstrict inequations are considered. \n",
      "Upper bounds for components of a minimal set of solutions and algorithms \n",
      "of construction of minimal generating sets of solutions for all types of \n",
      "systems are given. These criteria and the corresponding algorithms for \n",
      "constructing a minimal supporting set of solutions can be used in solving \n",
      "all the considered types of systems and systems of mixed types.\n",
      "  \"\"\"\n",
      "from nltk.tokenize import sent_tokenize\n",
      "texts = sent_tokenize(text)\n",
      "\n",
      "extract_candidate_chunks(text)\n",
      "extract_candidate_words(text)\n",
      "# print score_keyphrases_by_tfidf(texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['compatibility of systems', 'linear constraints', 'set of natural numbers', 'criteria of compatibility', 'system of linear diophantine equations', 'strict inequations', 'nonstrict inequations', 'upper', 'components', 'minimal set of solutions', 'algorithms of construction', 'sets of solutions', 'types of systems', 'criteria', 'corresponding algorithms', 'minimal supporting set of solutions', 'types of systems', 'systems of mixed types']\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "[]"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# LDA algorithm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import RegexpTokenizer\n",
      "from stop_words import get_stop_words\n",
      "\n",
      "from gensim import corpora, models\n",
      "import gensim\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
        "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_doc = \"in my own language.\\\n",
      "As a video uploader, this means you can reach\\\n",
      "to people all over the world,\\\n",
      "irrespective of language.\\\n",
      "[Hiroto, Bedhead]\\\n",
      "You can upload multiple tracks like English and French,\\\n",
      "and viewers can choose the track they like.\\\n",
      "[Toliver, Japanese Learner]\\\n",
      "For example, if you enjoy using YouTube in French,\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# preprocessing\n",
      "from nltk import sent_tokenize,tokenize\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "import re\n",
      "\n",
      "tokenizer = RegexpTokenizer(r'\\w+')\n",
      "# create English stop words list\n",
      "en_stop_words = get_stop_words('en')\n",
      "\n",
      "def data_clean(text, stemmer_type='english'):\n",
      "    if stemmer_type in ['english', 'porter']:\n",
      "        stemmer = SnowballStemmer(stemmer_type)\n",
      "    else:\n",
      "        stemmer = SnowballStemmer('english')\n",
      "    cleaned_texts = []\n",
      "    sentences = sent_tokenize(text)\n",
      "    for sentence in sentences:\n",
      "        # remove character name (need to be modify with specific file)\n",
      "        sentence = re.sub('(\\[.*\\])', '', sentence)\n",
      "        tokens = tokenizer.tokenize(sentence.lower())\n",
      "        # remove stop words\n",
      "        stopped_tokens = [i for i in tokens if not i in en_stop_words]\n",
      "        stemmed_tokens = [stemmer.stem(token) for token in stopped_tokens]\n",
      "        \n",
      "        cleaned_texts.append(stemmed_tokens)\n",
      "                \n",
      "    return cleaned_texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cleaned_texts = data_clean(test_doc, 'english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cleaned_texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "[[u'languag',\n",
        "  u'video',\n",
        "  u'upload',\n",
        "  u'mean',\n",
        "  u'can',\n",
        "  u'reachto',\n",
        "  u'peopl',\n",
        "  u'world',\n",
        "  u'irrespect',\n",
        "  u'languag'],\n",
        " [u'can',\n",
        "  u'upload',\n",
        "  u'multipl',\n",
        "  u'track',\n",
        "  u'like',\n",
        "  u'english',\n",
        "  u'french',\n",
        "  u'viewer',\n",
        "  u'can',\n",
        "  u'choos',\n",
        "  u'track',\n",
        "  u'like'],\n",
        " [u'exampl', u'enjoy', u'use', u'youtub', u'french']]"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lda_trainer(texts):\n",
      "    # turn our tokenized documents into a id <-> term dictionary\n",
      "    dictionary = corpora.Dictionary(texts)\n",
      "\n",
      "    # convert tokenized documents into a document-term matrix\n",
      "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "    # generate LDA model\n",
      "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
      "    \n",
      "    return ldamodel.print_topics(num_topics=2, num_words=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda_trainer(cleaned_texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 83,
       "text": [
        "[(0, u'0.101*\"french\" + 0.099*\"use\" + 0.099*\"enjoy\" + 0.099*\"exampl\"'),\n",
        " (1, u'0.110*\"can\" + 0.078*\"track\" + 0.078*\"like\" + 0.078*\"upload\"')]"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# noun or verb noun is more important\n",
      "# noun phrase\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# trained with entire subtitles \n",
      "# use tfidf find the most important words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}