{
 "metadata": {
  "name": "",
  "signature": "sha256:afa45b5b182530d0308e266403f543398a90743af2e3a4bba37082699d70ef51"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import RegexpTokenizer\n",
      "from stop_words import get_stop_words\n",
      "\n",
      "from gensim import corpora, models\n",
      "import gensim\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
        "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_doc = \"in my own language.\\\n",
      "As a video uploader, this means you can reach\\\n",
      "to people all over the world,\\\n",
      "irrespective of language.\\\n",
      "[Hiroto, Bedhead]\\\n",
      "You can upload multiple tracks like English and French,\\\n",
      "and viewers can choose the track they like.\\\n",
      "[Toliver, Japanese Learner]\\\n",
      "For example, if you enjoy using YouTube in French,\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# preprocessing\n",
      "from nltk import sent_tokenize,tokenize\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "import re\n",
      "\n",
      "tokenizer = RegexpTokenizer(r'\\w+')\n",
      "# create English stop words list\n",
      "en_stop_words = get_stop_words('en')\n",
      "\n",
      "def data_clean(text, stemmer_type='english'):\n",
      "    if stemmer_type in ['english', 'porter']:\n",
      "        stemmer = SnowballStemmer(stemmer_type)\n",
      "    else:\n",
      "        stemmer = SnowballStemmer('english')\n",
      "    cleaned_texts = []\n",
      "    sentences = sent_tokenize(text)\n",
      "    for sentence in sentences:\n",
      "        # remove character name (need to be modify with specific file)\n",
      "        sentence = re.sub('(\\[.*\\])', '', sentence)\n",
      "        tokens = tokenizer.tokenize(sentence.lower())\n",
      "        # remove stop words\n",
      "        stopped_tokens = [i for i in tokens if not i in en_stop_words]\n",
      "        stemmed_tokens = [stemmer.stem(token) for token in stopped_tokens]\n",
      "        \n",
      "        cleaned_texts.append(stemmed_tokens)\n",
      "                \n",
      "    return cleaned_texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cleaned_texts = data_clean(test_doc, 'english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cleaned_texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "[[u'languag',\n",
        "  u'video',\n",
        "  u'upload',\n",
        "  u'mean',\n",
        "  u'can',\n",
        "  u'reachto',\n",
        "  u'peopl',\n",
        "  u'world',\n",
        "  u'irrespect',\n",
        "  u'languag'],\n",
        " [u'can',\n",
        "  u'upload',\n",
        "  u'multipl',\n",
        "  u'track',\n",
        "  u'like',\n",
        "  u'english',\n",
        "  u'french',\n",
        "  u'viewer',\n",
        "  u'can',\n",
        "  u'choos',\n",
        "  u'track',\n",
        "  u'like'],\n",
        " [u'exampl', u'enjoy', u'use', u'youtub', u'french']]"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lda_trainer(texts):\n",
      "    # turn our tokenized documents into a id <-> term dictionary\n",
      "    dictionary = corpora.Dictionary(texts)\n",
      "\n",
      "    # convert tokenized documents into a document-term matrix\n",
      "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "    # generate LDA model\n",
      "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
      "    \n",
      "    return ldamodel.print_topics(num_topics=2, num_words=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda_trainer(cleaned_texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 83,
       "text": [
        "[(0, u'0.101*\"french\" + 0.099*\"use\" + 0.099*\"enjoy\" + 0.099*\"exampl\"'),\n",
        " (1, u'0.110*\"can\" + 0.078*\"track\" + 0.078*\"like\" + 0.078*\"upload\"')]"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# noun or verb noun is more important\n",
      "# noun phrase\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# trained with entire subtitles \n",
      "# use tfidf find the most important words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}