{
 "metadata": {
  "name": "",
  "signature": "sha256:766cc2af54e98a101079b06e755a2a2252999355f4eb682eb083c60cf43ebab5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# RAKE Algorithm\n",
      "\n",
      "Rapid Automatic Keyword Extraction\n",
      "\n",
      "feature: domain-independent, language-independent\n",
      "\n",
      "tend to extract longer phrase  \n",
      "a phrase almost not contain **punctuations** and **stop words**, thus split the sentence based on the punctuations and stop words.  \n",
      "\n",
      "Score of each phrase:  \n",
      "$$wordScore = wordDegree(w) / wordFrequency(w)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Adapted from: github.com/aneesha/RAKE/rake.py\n",
      "from __future__ import division\n",
      "import operator\n",
      "import nltk\n",
      "import string\n",
      "\n",
      "def isPunct(word):\n",
      "  return len(word) == 1 and word in string.punctuation\n",
      "\n",
      "def isNumeric(word):\n",
      "  try:\n",
      "    float(word) if '.' in word else int(word)\n",
      "    return True\n",
      "  except ValueError:\n",
      "    return False\n",
      "\n",
      "class RakeKeywordExtractor:\n",
      "\n",
      "  def __init__(self):\n",
      "    self.stopwords = set(nltk.corpus.stopwords.words())\n",
      "    self.top_fraction = 1 # consider top third candidate keywords by score\n",
      "\n",
      "  def _generate_candidate_keywords(self, sentences):\n",
      "    phrase_list = []\n",
      "    for sentence in sentences:\n",
      "      words = map(lambda x: \"|\" if x in self.stopwords else x,\n",
      "        nltk.word_tokenize(sentence.lower()))\n",
      "      phrase = []\n",
      "      for word in words:\n",
      "        if word == \"|\" or isPunct(word):\n",
      "          if len(phrase) > 0:\n",
      "            phrase_list.append(phrase)\n",
      "            phrase = []\n",
      "        else:\n",
      "          phrase.append(word)\n",
      "    return phrase_list\n",
      "\n",
      "  def _calculate_word_scores(self, phrase_list):\n",
      "    word_freq = nltk.FreqDist()\n",
      "    word_degree = nltk.FreqDist()\n",
      "    for phrase in phrase_list:\n",
      "      degree = len(filter(lambda x: not isNumeric(x), phrase)) - 1\n",
      "      for word in phrase:\n",
      "        word_freq[word] += 1\n",
      "        word_degree[word] += 1 # other words\n",
      "    for word in word_freq.keys():\n",
      "      word_degree[word] = word_degree[word] + word_freq[word] # itself\n",
      "    # word score = deg(w) / freq(w)\n",
      "    word_scores = {}\n",
      "    for word in word_freq.keys():\n",
      "      word_scores[word] = word_degree[word] / word_freq[word]\n",
      "    return word_scores\n",
      "\n",
      "  def _calculate_phrase_scores(self, phrase_list, word_scores):\n",
      "    phrase_scores = {}\n",
      "    for phrase in phrase_list:\n",
      "      phrase_score = 0\n",
      "      for word in phrase:\n",
      "        phrase_score += word_scores[word]\n",
      "      phrase_scores[\" \".join(phrase)] = phrase_score\n",
      "    return phrase_scores\n",
      "    \n",
      "  def extract(self, text, incl_scores=False):\n",
      "    sentences = nltk.sent_tokenize(text)\n",
      "    phrase_list = self._generate_candidate_keywords(sentences)\n",
      "    word_scores = self._calculate_word_scores(phrase_list)\n",
      "    phrase_scores = self._calculate_phrase_scores(\n",
      "      phrase_list, word_scores)\n",
      "    sorted_phrase_scores = sorted(phrase_scores.iteritems(),\n",
      "      key=operator.itemgetter(1), reverse=True)\n",
      "    n_phrases = len(sorted_phrase_scores)\n",
      "    if incl_scores:\n",
      "      return sorted_phrase_scores[0:int(n_phrases/self.top_fraction)]\n",
      "    else:\n",
      "      return map(lambda x: x[0],\n",
      "        sorted_phrase_scores[0:int(n_phrases/self.top_fraction)])\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test():\n",
      "  rake = RakeKeywordExtractor()\n",
      "  keywords = rake.extract(\"\"\"\n",
      "Compatibility of systems of linear constraints over the set of natural \n",
      "numbers. Criteria of compatibility of a system of linear Diophantine \n",
      "equations, strict inequations, and nonstrict inequations are considered. \n",
      "Upper bounds for components of a minimal set of solutions and algorithms \n",
      "of construction of minimal generating sets of solutions for all types of \n",
      "systems are given. These criteria and the corresponding algorithms for \n",
      "constructing a minimal supporting set of solutions can be used in solving \n",
      "all the considered types of systems and systems of mixed types.\n",
      "  \"\"\", incl_scores=True)\n",
      "  print keywords\n",
      "  \n",
      "    \n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('linear diophantine equations', 6.0), ('minimal supporting set', 6.0), ('minimal generating sets', 6.0), ('minimal set', 4.0), ('upper bounds', 4.0), ('linear constraints', 4.0), ('corresponding algorithms', 4.0), ('natural numbers', 4.0), ('nonstrict inequations', 4.0), ('considered types', 4.0), ('strict inequations', 4.0), ('mixed types', 4.0), ('set', 2.0), ('constructing', 2.0), ('solutions', 2.0), ('given', 2.0), ('solving', 2.0), ('system', 2.0), ('systems', 2.0), ('criteria', 2.0), ('compatibility', 2.0), ('used', 2.0), ('construction', 2.0), ('types', 2.0), ('considered', 2.0), ('algorithms', 2.0), ('components', 2.0)]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample = \"in my own language.\\\n",
      "As a video uploader, this means you can reach\\\n",
      "to people all over the world,\\\n",
      "irrespective of language.\\\n",
      "[Hiroto, Bedhead]\\\n",
      "You can upload multiple tracks like English and French,\\\n",
      "and viewers can choose the track they like.\\\n",
      "[Toliver, Japanese Learner]\\\n",
      "For example, if you enjoy using YouTube in French,\"\n",
      "rake = RakeKeywordExtractor()\n",
      "keywords = rake.extract(sample, incl_scores=True)\n",
      "\n",
      "print keywords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('upload multiple tracks like english', 10.0), ('enjoy using youtube', 6.0), ('video uploader', 4.0), ('reachto people', 4.0), ('japanese learner', 4.0), ('toliver', 2.0), ('like', 2.0), ('language', 2.0), ('means', 2.0), ('bedhead', 2.0), ('choose', 2.0), ('french', 2.0), ('track', 2.0), ('hiroto', 2.0), ('irrespective', 2.0), ('world', 2.0), ('example', 2.0), ('viewers', 2.0), ('language.as', 2.0)]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# TextRank Algorithm\n",
      "\n",
      "https://github.com/davidadamojr/TextRank\n",
      "\n",
      "graph-based ranking algorithm  \n",
      "rank words based on their associations in the graph  \n",
      "has best performance when only nouns and adjectives are selected as potential keywords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "From this paper: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
      "External dependencies: nltk, numpy, networkx\n",
      "Based on https://gist.github.com/voidfiles/1646117\n",
      "\"\"\"\n",
      "\n",
      "import io\n",
      "import nltk\n",
      "import itertools\n",
      "from operator import itemgetter\n",
      "import networkx as nx\n",
      "import os\n",
      "\n",
      "#apply syntactic filters based on POS tags\n",
      "def filter_for_tags(tagged, tags=['NN', 'JJ', 'NNP']):\n",
      "    return [item for item in tagged if item[1] in tags]\n",
      "\n",
      "def normalize(tagged):\n",
      "    return [(item[0].replace('.', ''), item[1]) for item in tagged]\n",
      "\n",
      "def unique_everseen(iterable, key=None):\n",
      "    \"List unique elements, preserving order. Remember all elements ever seen.\"\n",
      "    # unique_everseen('AAAABBBCCDAABBB') --> A B C D\n",
      "    # unique_everseen('ABBCcAD', str.lower) --> A B C D\n",
      "    seen = set()\n",
      "    seen_add = seen.add\n",
      "    if key is None:\n",
      "        for element in itertools.ifilterfalse(seen.__contains__, iterable):\n",
      "            seen_add(element)\n",
      "            yield element\n",
      "    else:\n",
      "        for element in iterable:\n",
      "            k = key(element)\n",
      "            if k not in seen:\n",
      "                seen_add(k)\n",
      "                yield element\n",
      "\n",
      "def lDistance(firstString, secondString):\n",
      "    \"Function to find the Levenshtein distance between two words/sentences - gotten from http://rosettacode.org/wiki/Levenshtein_distance#Python\"\n",
      "    if len(firstString) > len(secondString):\n",
      "        firstString, secondString = secondString, firstString\n",
      "    distances = range(len(firstString) + 1)\n",
      "    for index2, char2 in enumerate(secondString):\n",
      "        newDistances = [index2 + 1]\n",
      "        for index1, char1 in enumerate(firstString):\n",
      "            if char1 == char2:\n",
      "                newDistances.append(distances[index1])\n",
      "            else:\n",
      "                newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1])))\n",
      "        distances = newDistances\n",
      "    return distances[-1]\n",
      "\n",
      "def buildGraph(nodes):\n",
      "    \"nodes - list of hashables that represents the nodes of the graph\"\n",
      "    gr = nx.Graph() #initialize an undirected graph\n",
      "    gr.add_nodes_from(nodes)\n",
      "    nodePairs = list(itertools.combinations(nodes, 2))\n",
      "\n",
      "    #add edges to the graph (weighted by Levenshtein distance)\n",
      "    for pair in nodePairs:\n",
      "        firstString = pair[0]\n",
      "        secondString = pair[1]\n",
      "        levDistance = lDistance(firstString, secondString)\n",
      "        gr.add_edge(firstString, secondString, weight=levDistance)\n",
      "\n",
      "    return gr\n",
      "\n",
      "def extractKeyphrases(text):\n",
      "    #tokenize the text using nltk\n",
      "    wordTokens = nltk.word_tokenize(text)\n",
      "\n",
      "    #assign POS tags to the words in the text\n",
      "    tagged = nltk.pos_tag(wordTokens)\n",
      "    textlist = [x[0] for x in tagged]\n",
      "    \n",
      "    tagged = filter_for_tags(tagged)\n",
      "    tagged = normalize(tagged)\n",
      "\n",
      "    unique_word_set = unique_everseen([x[0] for x in tagged])\n",
      "    word_set_list = list(unique_word_set)\n",
      "\n",
      "   #this will be used to determine adjacent words in order to construct keyphrases with two words\n",
      "\n",
      "    graph = buildGraph(word_set_list)\n",
      "\n",
      "    #pageRank - initial value of 1.0, error tolerance of 0,0001, \n",
      "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
      "\n",
      "    #most important words in ascending order of importance\n",
      "    keyphrases = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
      "\n",
      "    #the number of keyphrases returned will be relative to the size of the text (a third of the number of vertices)\n",
      "    aThird = len(word_set_list) / 3\n",
      "    keyphrases = keyphrases[0:aThird+1]\n",
      "\n",
      "    #take keyphrases with multiple words into consideration as done in the paper - if two words are adjacent in the text and are selected as keywords, join them\n",
      "    #together\n",
      "    modifiedKeyphrases = set([])\n",
      "    dealtWith = set([]) #keeps track of individual keywords that have been joined to form a keyphrase\n",
      "    i = 0\n",
      "    j = 1\n",
      "    while j < len(textlist):\n",
      "        firstWord = textlist[i]\n",
      "        secondWord = textlist[j]\n",
      "        if firstWord in keyphrases and secondWord in keyphrases:\n",
      "            keyphrase = firstWord + ' ' + secondWord\n",
      "            modifiedKeyphrases.add(keyphrase)\n",
      "            dealtWith.add(firstWord)\n",
      "            dealtWith.add(secondWord)\n",
      "        else:\n",
      "            if firstWord in keyphrases and firstWord not in dealtWith: \n",
      "                modifiedKeyphrases.add(firstWord)\n",
      "\n",
      "            #if this is the last word in the text, and it is a keyword,\n",
      "            #it definitely has no chance of being a keyphrase at this point    \n",
      "            if j == len(textlist)-1 and secondWord in keyphrases and secondWord not in dealtWith:\n",
      "                modifiedKeyphrases.add(secondWord)\n",
      "        \n",
      "        i = i + 1\n",
      "        j = j + 1\n",
      "        \n",
      "    return modifiedKeyphrases\n",
      "\n",
      "def extractSentences(text):\n",
      "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "    sentenceTokens = sent_detector.tokenize(text.strip())\n",
      "    graph = buildGraph(sentenceTokens)\n",
      "\n",
      "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
      "\n",
      "    #most important sentences in ascending order of importance\n",
      "    sentences = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
      "\n",
      "    #return a 100 word summary\n",
      "    summary = ' '.join(sentences)\n",
      "    summaryWords = summary.split()\n",
      "    summaryWords = summaryWords[0:101]\n",
      "    summary = ' '.join(summaryWords)\n",
      "\n",
      "    return summary\n",
      "\n",
      "def writeFiles(summary, keyphrases, fileName):\n",
      "    \"outputs the keyphrases and summaries to appropriate files\"\n",
      "    print \"Generating output to \" + 'keywords/' + fileName\n",
      "    keyphraseFile = io.open('keywords/' + fileName, 'w')\n",
      "    for keyphrase in keyphrases:\n",
      "        keyphraseFile.write(keyphrase + '\\n')\n",
      "    keyphraseFile.close()\n",
      "\n",
      "    print \"Generating output to \" + 'summaries/' + fileName\n",
      "    summaryFile = io.open('summaries/' + fileName, 'w')\n",
      "    summaryFile.write(summary)\n",
      "    summaryFile.close()\n",
      "\n",
      "    print \"-\"\n",
      "\n",
      "\n",
      "#retrieve each of the articles\n",
      "articles = os.listdir(\"articles\")\n",
      "for article in articles:\n",
      "    print 'Reading articles/' + article\n",
      "    articleFile = io.open('articles/' + article, 'r')\n",
      "    text = articleFile.read()\n",
      "    keyphrases = extractKeyphrases(text)\n",
      "    summary = extractSentences(text)\n",
      "    writeFiles(summary, keyphrases, article)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OSError",
       "evalue": "[Errno 2] No such file or directory: 'articles'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-d8be38aa84f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;31m#retrieve each of the articles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m \u001b[0marticles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"articles\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'Reading articles/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'articles'"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# LDA algorithm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import RegexpTokenizer\n",
      "from stop_words import get_stop_words\n",
      "\n",
      "from gensim import corpora, models\n",
      "import gensim\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
        "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_doc = \"in my own language.\\\n",
      "As a video uploader, this means you can reach\\\n",
      "to people all over the world,\\\n",
      "irrespective of language.\\\n",
      "[Hiroto, Bedhead]\\\n",
      "You can upload multiple tracks like English and French,\\\n",
      "and viewers can choose the track they like.\\\n",
      "[Toliver, Japanese Learner]\\\n",
      "For example, if you enjoy using YouTube in French,\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# preprocessing\n",
      "from nltk import sent_tokenize,tokenize\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "import re\n",
      "\n",
      "tokenizer = RegexpTokenizer(r'\\w+')\n",
      "# create English stop words list\n",
      "en_stop_words = get_stop_words('en')\n",
      "\n",
      "def data_clean(text, stemmer_type='english'):\n",
      "    if stemmer_type in ['english', 'porter']:\n",
      "        stemmer = SnowballStemmer(stemmer_type)\n",
      "    else:\n",
      "        stemmer = SnowballStemmer('english')\n",
      "    cleaned_texts = []\n",
      "    sentences = sent_tokenize(text)\n",
      "    for sentence in sentences:\n",
      "        # remove character name (need to be modify with specific file)\n",
      "        sentence = re.sub('(\\[.*\\])', '', sentence)\n",
      "        tokens = tokenizer.tokenize(sentence.lower())\n",
      "        # remove stop words\n",
      "        stopped_tokens = [i for i in tokens if not i in en_stop_words]\n",
      "        stemmed_tokens = [stemmer.stem(token) for token in stopped_tokens]\n",
      "        \n",
      "        cleaned_texts.append(stemmed_tokens)\n",
      "                \n",
      "    return cleaned_texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cleaned_texts = data_clean(test_doc, 'english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cleaned_texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "[[u'languag',\n",
        "  u'video',\n",
        "  u'upload',\n",
        "  u'mean',\n",
        "  u'can',\n",
        "  u'reachto',\n",
        "  u'peopl',\n",
        "  u'world',\n",
        "  u'irrespect',\n",
        "  u'languag'],\n",
        " [u'can',\n",
        "  u'upload',\n",
        "  u'multipl',\n",
        "  u'track',\n",
        "  u'like',\n",
        "  u'english',\n",
        "  u'french',\n",
        "  u'viewer',\n",
        "  u'can',\n",
        "  u'choos',\n",
        "  u'track',\n",
        "  u'like'],\n",
        " [u'exampl', u'enjoy', u'use', u'youtub', u'french']]"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lda_trainer(texts):\n",
      "    # turn our tokenized documents into a id <-> term dictionary\n",
      "    dictionary = corpora.Dictionary(texts)\n",
      "\n",
      "    # convert tokenized documents into a document-term matrix\n",
      "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "    # generate LDA model\n",
      "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
      "    \n",
      "    return ldamodel.print_topics(num_topics=2, num_words=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda_trainer(cleaned_texts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 83,
       "text": [
        "[(0, u'0.101*\"french\" + 0.099*\"use\" + 0.099*\"enjoy\" + 0.099*\"exampl\"'),\n",
        " (1, u'0.110*\"can\" + 0.078*\"track\" + 0.078*\"like\" + 0.078*\"upload\"')]"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# noun or verb noun is more important\n",
      "# noun phrase\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# trained with entire subtitles \n",
      "# use tfidf find the most important words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}